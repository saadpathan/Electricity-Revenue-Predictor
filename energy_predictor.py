# -*- coding: utf-8 -*-
"""Energy Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xVQPAmAPG0M0y_8899Nsq38yl_-avH5

# **Data Preprocessing**

**1. Data Cleaning:**
Check for missing values, outliers, and inconsistencies in the dataset and handle them appropriately. Missing values can be imputed or dropped based on the extent of missingness and their impact on the analysis.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')

data.shape

# Load the dataset
file_path = '/content/drive/MyDrive/Machine Learning Project/Electricity Consumption by Year Updated.csv'
data = pd.read_csv(file_path)
print(data.head())  # Display the first few rows to understand the data
print(data.describe())  # Get a statistical summary to check for outliers

data.head(10)

data.iloc[741 : 760]

data.tail(10)

data.info()

data.isnull().sum()

data.Utility.value_counts()

# Check for missing values
print(data.isnull().sum())

# Handling missing values - Option 1: Imputation
imputer = SimpleImputer(strategy='mean')  # Or median, mode (most_frequent)
data['column_name'] = imputer.fit_transform(data[['column_name']].values)

# Handling missing values - Option 2: Dropping
data.dropna(inplace=True)  # You can specify `subset=['column1', 'column2']` to only drop rows based on specific columns

# Handling outliers - Example using IQR
Q1 = data['numerical_column'].quantile(0.25)
Q3 = data['numerical_column'].quantile(0.75)
IQR = Q3 - Q1
data = data[~((data['numerical_column'] < (Q1 - 1.5 * IQR)) | (data['numerical_column'] > (Q3 + 1.5 * IQR)))]

# Example: Extracting day of the week, time of the day from 'timestamp' column
data['day_of_week'] = pd.to_datetime(data['timestamp']).dt.dayofweek
data['hour'] = pd.to_datetime(data['timestamp']).dt.hour

# Adding seasonal information based on month
data['season'] = data['timestamp'].apply(lambda date: (date.month%12 + 3)//3)

# Normalizing numerical features
scaler = StandardScaler()
data[['numerical_feature1', 'numerical_feature2']] = scaler.fit_transform(data[['numerical_feature1', 'numerical_feature2']])

# One-hot encoding
encoder = OneHotEncoder(sparse=False)
encoded_features = encoder.fit_transform(data[['categorical_column']])
data = pd.concat([data, pd.DataFrame(encoded_features, columns=encoder.get_feature_names(['categorical_column']))], axis=1)
data.drop(['categorical_column'], axis=1, inplace=True)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Define the pipeline for numerical features
numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_features.remove('Average No. of Customers - Sales for Resale')  # Exclude the column with missing values for separate treatment

numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values
    ('scaler', StandardScaler())  # Scale features
])

# Pipeline for categorical features
categorical_features = ['Type of Utility', 'Utility']
categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values
    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables
])

# Combine pipelines into a single ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ])

# Apply the preprocessing pipeline to the data
data_preprocessed = preprocessor.fit_transform(data)

# If you wish to see the processed data as a DataFrame:
processed_columns = numerical_features + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names(categorical_features))
data_processed_df = pd.DataFrame(data_preprocessed, columns=processed_columns)

print(data_processed_df.head())