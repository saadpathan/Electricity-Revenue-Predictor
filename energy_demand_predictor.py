# -*- coding: utf-8 -*-
"""Energy Demand Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_xVQPAmAPG0M0y_8899Nsq38yl_-avH5

# **1. Project Designing**

# **2. Data Mining**

# **3. Data Preprocessing**
"""

# Line Wrapping in Collaboratory Google results
from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

"""**1. Data Cleaning:**
Check for missing values, outliers, and inconsistencies in the dataset and handle them appropriately. Missing values can be imputed or dropped based on the extent of missingness and their impact on the analysis.
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries for analysis and visualisation
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
# %matplotlib inline

# To import datetime library
from datetime import datetime
import datetime as dt

# Library of warnings would assist in ignoring warnings issued
import warnings
warnings.filterwarnings('ignore')

# Import necessary statistical libraries
import scipy.stats as stats
import statsmodels.api as sm
from scipy.stats import norm

# Import libraries for ML-Model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import  MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.linear_model import  LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import  BaggingRegressor, ExtraTreesRegressor, RandomForestRegressor,StackingRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor
import lightgbm
from lightgbm import LGBMRegressor
# Libraries for save the model
import pickle

# Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')

# Load the dataset
file_path = '/content/drive/MyDrive/Machine Learning Project/electricity_consumption_data.csv'

df = pd.read_csv(file_path)

# Display the shape of the data
df.shape

# Display the first few rows to understand the data
print(df.head())

df.head(5)

df.iloc[745 : 751]

df.tail(5)

df.info()

# Determine the datatype of Each Column
df.dtypes

# Get a statistical summary to check for outliers
print(df.describe())

# Get duplicates count for each unique row
dup_Count =  len(df)-len(df.drop_duplicates())

# There is no duplicate values in the dataframe
dup_count1 = df[df.duplicated()].shape
dup_count1

# Find the missing values of each column
null_values = df.isnull().sum()

# Visualizing the missing values
plt.figure(figsize=(10,10))
sns.displot(
    data=df.isna().melt(value_name="missing"),
    y="variable",
    hue="missing",
    multiple="fill",
    aspect=1.25
)
plt.savefig("visualizing_missing_data_with_barplot_Seaborn_distplot.png", dpi=100)

# Remove all rows with missing data
data = df.dropna()
data.isna().sum()

"""# **4. Dataset Information**

RY - Reporting Year

ToU - Type of Utility

U - Utility

ORoRS - Operating Revenues of Residential Sales

ORoCIS - Operating Revenues of Commercial & Industrial Sales

ORoSR - Operating Revenues of Sales for Resale

ORoAOS - Operating Revenues of All Other Sales

ASforR - Amount Sold for Residential in MWh

ASforCI - Amount Sold for Commercial & Industrial in MWh

ASforSR - Amount Sold for Sales for Resale in MWh

ASforAO - Amount Sold for All Other in MWh

ANoCR - Average No. of Customers in Residential

ANoCCI - Average No. of Customers in Commercial & Industrial

ANoCSR - Average No. of Customers in Sales for Resale

ANoCAO - Average No. of Customers in All Other
"""

# Show all columns
df.columns

df_energy = df.copy()

# Convert to DataFrame
df_energy = pd.DataFrame(data)

# Apply One-Hot Encoding
df_energy = pd.get_dummies(df_energy, columns=['ToU', 'U'])

print("DataFrame after One-Hot Encoding:")
print(df_energy)

# Rename the columns
df_rename = df.copy()
df_rename.rename(columns={'RY': 'reporting_year', 'ToU':'utility_type', 'U':'utility', 'ORoRS ': 'residential_revenues', 'ORoCIS':'commercial_revenues',
       'ORoSR':'resale_revenues', 'ORoAOS':'other_revenues', 'ASforR ':'residential_sales', 'ASforCI':'commercial_sales', 'ASforSR':'resale_sales', 'ASforAO':'other_sales'
       ,'ANoCR':'residential_customers', 'ANoCCI':'commercial_customers', 'ANoCSR':'resale_customers', 'ANoCAO':'other_customers'},inplace = True)

df_rename.columns

# Check Unique Values for each variable
def get_unqiuevalues(df1):
    unique_values=df1.apply(pd.Series.unique)
    return unique_values

unq_values = get_unqiuevalues(df)

for i in df.columns.tolist():
  print("No. of unique values in ",i,"is",df[i].nunique())

# Separate columns in list for better analysis
gen_cols=['reporting_year', 'utility_type', 'utility']
rev_cols=['residential_revenues', 'commercial_revenues', 'resale_revenues', 'other_revenues']
sal_cols=['residential_sales', 'commercial_sales', 'resale_sales', 'other_sales']
cus_cols=['residential_customers', 'commercial_customers', 'resale_customers','other_customers']

# random_col = ["rv1","rv2"]

"""# **5. Data Vizualization**"""

# Chart - 01 visualization
# Dependent varaible "ORoCIS - commercial_revenues"
plt.figure(figsize=(5,5))
sns.distplot(df_energy['ORoCIS'], color = 'Blue')

# Chart - 02 visualization
# Dependent varaible "ASforR - residential_sales"
plt.figure(figsize=(5,5))
sns.distplot(df_energy['ASforR'], color = 'Blue')

# Chart - 03 visualization
# Dependent varaible "ANoCCI - commercial_customers"
plt.figure(figsize=(5,5))
sns.distplot(df_energy['ANoCCI'], color = 'Blue')

# Display the heatmap
data['ToU'] = data['ToU'].astype('category').cat.codes
data['U'] = data['U'].astype('category').cat.codes

correlation_matrix = data.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)

plt.title('Correlation Matrix Heatmap')
plt.show()

# Handling outliers & outlier treatments
df = df_energy.copy()
col_list = list(df.describe().columns)

# Find the outliers using boxplot
plt.figure(figsize=(25, 20))
plt.suptitle("Box Plot", fontsize=18, y=0.95)

for n, ticker in enumerate(col_list):

    ax = plt.subplot(8, 4, n + 1)

    plt.subplots_adjust(hspace=0.5, wspace=0.2)

    sns.boxplot(x=df[ticker],color='pink', ax = ax)

    ax.set_title(ticker.upper())

"""# **6. Feature Selection**"""

# Feature Selection using PCA
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

df_energy = pd.DataFrame(data)

df_energy = pd.get_dummies(df_energy, columns=['ToU', 'U'])

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_energy)

# Set the number of principal components
pca = PCA(n_components=5)
principal_components = pca.fit_transform(scaled_features)

pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])

print("PCA result:")
print(pca_df)

print("Explained variance ratio by each principal component:")
print(pca.explained_variance_ratio_)

# Convert to DataFrame
df_energy = pd.DataFrame(data)

# Convert categorical variables to numerical using One-Hot Encoding
df_energy = pd.get_dummies(df_energy, columns=['ToU', 'U'])

# Standardize the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_energy)

# Apply PCA
pca = PCA(n_components=5)  # Set the number of principal components
principal_components = pca.fit_transform(scaled_features)

# Create a DataFrame with the principal components
pca_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])

# Select PC1 as the feature
X = pca_df[['PC1']]

# Assuming ORoRS as the dependent variable for regression
y = df_energy['ORoRS']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting datasets
print("Shapes of the datasets:")
print(f"X_train: {X_train.shape}")
print(f"X_test: {X_test.shape}")
print(f"y_train: {y_train.shape}")
print(f"y_test: {y_test.shape}")

"""# **7. Model Selection**"""

# Select PC1 as the feature
X = pca_df[['PC1']]

# Assuming ORoRS as the dependent variable for regression
y = df_energy['ORoRS']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.metrics import mean_squared_error, r2_score

# Initialize and train the Linear Regression model
linear_regressor = LinearRegression()
linear_regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = linear_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Linear Regression Model Evaluation:")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

# Plotting the results
import matplotlib.pyplot as plt

plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted')
plt.xlabel('PC1')
plt.ylabel('ORoRS')
plt.title('Linear Regression: Actual vs Predicted')
plt.legend()
plt.show()

from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

# Split the dataset into training and testing sets
X_train_lasso, X_test_lasso, y_train_lasso, y_test_lasso = train_test_split(
    pca_df, df_energy['ORoRS'], test_size=0.2, random_state=42
)

# Initialize and train the Lasso Regression model
lasso_regressor = Lasso()
parameters = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}  # Define a grid of hyperparameters
lasso_grid = GridSearchCV(lasso_regressor, parameters, cv=5)
lasso_grid.fit(X_train_lasso, y_train_lasso)

# Select the best hyperparameters
best_alpha = lasso_grid.best_params_['alpha']

# Initialize Lasso Regression with the best alpha
lasso_regressor = Lasso(alpha=best_alpha)
lasso_regressor.fit(X_train_lasso, y_train_lasso)

# Predict on the test set
y_pred_lasso = lasso_regressor.predict(X_test_lasso)

# Evaluate the model
mse_lasso = mean_squared_error(y_test_lasso, y_pred_lasso)
r2_lasso = r2_score(y_test_lasso, y_pred_lasso)

print("Lasso Regression Model Evaluation:")
print(f"Mean Squared Error (MSE): {mse_lasso}")
print(f"R-squared (R2): {r2_lasso}")

from sklearn.linear_model import Ridge

# Split the dataset into training and testing sets
X_train_ridge, X_test_ridge, y_train_ridge, y_test_ridge = train_test_split(
    pca_df, df_energy['ORoRS'], test_size=0.2, random_state=42
)

# Initialize and train the Ridge Regression model
ridge_regressor = Ridge()
parameters = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}  # Define a grid of hyperparameters
ridge_grid = GridSearchCV(ridge_regressor, parameters, cv=5)
ridge_grid.fit(X_train_ridge, y_train_ridge)

# Select the best hyperparameters
best_alpha_ridge = ridge_grid.best_params_['alpha']

# Initialize Ridge Regression with the best alpha
ridge_regressor = Ridge(alpha=best_alpha_ridge)
ridge_regressor.fit(X_train_ridge, y_train_ridge)

# Predict on the test set
y_pred_ridge = ridge_regressor.predict(X_test_ridge)

# Evaluate the model
mse_ridge = mean_squared_error(y_test_ridge, y_pred_ridge)
r2_ridge = r2_score(y_test_ridge, y_pred_ridge)

print("Ridge Regression Model Evaluation:")
print(f"Mean Squared Error (MSE): {mse_ridge}")
print(f"R-squared (R2): {r2_ridge}")

from sklearn.linear_model import LogisticRegression

# Assuming ORoRS as the dependent variable for logistic regression
y_logistic = (df_energy['ORoRS'] > df_energy['ORoRS'].mean()).astype(int)  # Convert to binary for logistic regression

# Split the dataset into training and testing sets
X_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic = train_test_split(
    pca_df, y_logistic, test_size=0.2, random_state=42
)

# Initialize and train the Logistic Regression model
logistic_regressor = LogisticRegression(random_state=42)
logistic_regressor.fit(X_train_logistic, y_train_logistic)

# Predict on the test set
y_pred_logistic = logistic_regressor.predict(X_test_logistic)

# Evaluate the model
accuracy = logistic_regressor.score(X_test_logistic, y_test_logistic)

print("Logistic Regression Model Evaluation:")
print(f"Accuracy: {accuracy}")

# Note: Logistic regression does not have MSE or R-squared as evaluation metrics

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Create confusion matrix
cm = confusion_matrix(y_test_logistic, y_pred_logistic)

# Plot confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True,
            xticklabels=['Actual Negative', 'Actual Positive'],
            yticklabels=['Predicted Negative', 'Predicted Positive'])
plt.xlabel('True Label')
plt.ylabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import RandomForestRegressor

# Select PC1 as the feature
X = pca_df[['PC1']]

# Assuming ORoRS as the dependent variable for regression
y = df_energy['ORoRS']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Random Forest Regressor Model Evaluation:")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")

# Plotting the results
import matplotlib.pyplot as plt

plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.scatter(X_test, y_pred, color='red', label='Predicted')
plt.xlabel('PC1')
plt.ylabel('ORoRS')
plt.title('Random Forest Regressor: Actual vs Predicted')
plt.legend()
plt.show()

"""# **8. Model Evaluation**

Based on the evaluation metrics from above five models, the Random Forest Regressor model demonstrates superior performance compared to the other models. It achieves this by exhibiting the lowest Mean Squared Error (MSE) and the highest R-squared value among all models. These metrics indicate that the Random Forest Regressor provides more accurate predictions and better explains the variance in the target variable compared to the other regression models.
"""

import matplotlib.pyplot as plt

# Evaluation results for each model
models = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Logistic Regression', 'Random Forest Regressor']
mse_values = [231588770533065.2, 43439706878974.95, 45470395263320.445, -1, 5296837027584.261]  # Replace None with -1 for Logistic Regression
r2_values = [0.9067015625486912, 0.9824997698902996, 0.9816816816350968, -1, 0.9978661028469975]  # Replace None with -1 for Logistic Regression
accuracy = [-1, -1, -1, 0.9553264604810997, -1]  # Replace None with -1 for regression models

# Plotting
fig, axs = plt.subplots(3, figsize=(10, 15))

# MSE comparison
axs[0].bar(models, mse_values, color=['blue', 'orange', 'green', 'red', 'purple'])
axs[0].set_title('Mean Squared Error (MSE) Comparison')
axs[0].set_ylabel('MSE')

# R-squared comparison
axs[1].bar(models, r2_values, color=['blue', 'orange', 'green', 'red', 'purple'])
axs[1].set_title('R-squared (R2) Comparison')
axs[1].set_ylabel('R-squared')

# Accuracy comparison
axs[2].bar(models, accuracy, color=['blue', 'orange', 'green', 'red', 'purple'])
axs[2].set_title('Accuracy Comparison')
axs[2].set_ylabel('Accuracy')

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Linear Regression Model Evaluation:**

Mean Squared Error (MSE): 231588770533065.2

R-squared (R2): 0.9067015625486912



**Lasso Regression Model Evaluation:**

Mean Squared Error (MSE): 43439706878974.95

R-squared (R2): 0.9824997698902996



**Ridge Regression Model Evaluation:**

Mean Squared Error (MSE): 45470395263320.445

R-squared (R2): 0.9816816816350968



**Logistic Regression Model Evaluation:**

Accuracy: 0.9553264604810997



**Random Forest Regressor Model Evaluation:**

Mean Squared Error (MSE): 5296837027584.261

R-squared (R2): 0.9978661028469975



**Best Model**

Random Forest Regressor

Mean Squared Error (MSE): 5296837027584.261

R-squared (R2): 0.9978661028469975

# **9. Conclusion**

Based on the evaluation of the different models, several key findings can be concluded:

1. **Linear Regression**: The linear regression model performed the poorest among the models evaluated, with a high Mean Squared Error (MSE) of approximately 231588770533065.2 and a moderate R-squared (R2) value of 0.9067. This indicates that the linear model did not effectively capture the relationships in the data.

2. **Lasso Regression**: The Lasso regression model showed significant improvement over the linear regression model, with a much lower MSE of 43439706878974.95 and a higher R-squared value of 0.9825. This indicates that the Lasso model was able to reduce overfitting and improve the overall fit of the model.

3. **Ridge Regression**: Similarly, the Ridge regression model also outperformed the linear regression model, with a lower MSE of 45470395263320.445 and a higher R-squared value of 0.9817. The Ridge model helped mitigate multicollinearity and performed better than the linear model.

4. **Logistic Regression**: The logistic regression model achieved a high accuracy of 0.9553, indicating its effectiveness in classifying instances. However, direct comparison with regression models based on MSE or R-squared is not appropriate due to the nature of the logistic regression task.

5. **Random Forest Regressor**: The Random Forest regressor performed the best among all models, with the lowest MSE of 5296837027584.261 and the highest R-squared value of 0.9979. This indicates that the Random Forest model provided the most accurate predictions and best explained the variance in the target variable.

In conclusion, the Random Forest Regressor model is recommended for this project, as it demonstrated the best performance in terms of predictive accuracy and model fit. The Lasso and Ridge regression models also showed improvements over the linear regression model and could be considered as alternatives depending on specific project requirements. The logistic regression model, while effective for classification tasks, is not directly comparable to regression models.

**Congratulations on completing the project! 🎉 Your hard work and dedication have truly paid off.🎉🎉**
"""